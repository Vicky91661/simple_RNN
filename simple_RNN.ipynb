{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END TO END DEEP LEARNING PROJECT USING SIMPLE RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb dataset\n",
    "\n",
    "max_features = 10000 #Vocabulary size\n",
    "(X_train,y_train),(X_test,y_test) = imdb.load_data(num_words = max_features)\n",
    "\n",
    "\n",
    "#Print the shape of the data \n",
    "print(f'Training data shape: {X_train.shape},Training labels shape: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0] # its a onehot representtaion of the 1st sentence , every index is leass than 10,000. Since Vocabulary size = 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect a sample review and its label\n",
    "sample_review = X_train[0]\n",
    "sample_label = y_train[0]\n",
    "\n",
    "print(f'Sample review (as integer) : {sample_review}')\n",
    "print(f'Sample label: { sample_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mapping of words index back to the words(for our understanding)\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "# word_index is a dictonary ,here word as a key and number as value. But we to reverse it .\n",
    "word_index \n",
    "# make number as key and word as the value\n",
    "reverse_word_index = {value:key for key, value in word_index.items()}\n",
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decode_review = ''\n",
    "for index in sample_review:\n",
    "    if index <3:\n",
    "        decode_review=decode_review+' '+'?'\n",
    "        print(\"?\")\n",
    "    else:\n",
    "        decode_review=decode_review +' '+ reverse_word_index[index-3] \n",
    "        print(f' {reverse_word_index[index-3]}')\n",
    "\n",
    "decode_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of doing above this is \n",
    "\n",
    "decode_review  = ' '.join([reverse_word_index.get(i-3,'?') for i in sample_review])\n",
    "decode_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "max_len = 500\n",
    "\n",
    "# pre padding technique is used to make all the input size equal to the 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Simple RNN\n",
    "model = Sequential()\n",
    "# Add Embedding Layer to the model :=> which is Responsible to convert each word to vector of dimension, input dimension = 500 and output dimension = 128\n",
    "model.add(Embedding(max_features,128,input_length =max_len))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add simple RNN to the model of with  128 hidden layer\n",
    "model.add(SimpleRNN(128,activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add outpurt layer to the model\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when compiling the model we need to define the optimizer function, loss function we have to use \n",
    "# Here i'm using adam optimizer function , loss = binary_crossentropy\n",
    "model.compile(optimizer = 'adam',loss= 'binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Use Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# monitore the validation loss and wait for 10 epocs and store the weights whenever you find the best validation loss\n",
    "earlystopping = EarlyStopping(monitor = 'val_loss',patience = 10, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model with early stopping\n",
    "model.fit(\n",
    "    X_train,y_train,epochs =10, batch_size = 32,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [earlystopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model file\n",
    "model.save('simple_rnn_imdb.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
